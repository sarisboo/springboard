{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TL;DR In depth Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikihow_subset = pd.read_csv('./datasets/wikihow_sep_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_summary</th>\n",
       "      <th>words</th>\n",
       "      <th>title</th>\n",
       "      <th>sentence_len</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>title_similarity</th>\n",
       "      <th>title_sim_categories</th>\n",
       "      <th>help</th>\n",
       "      <th>...</th>\n",
       "      <th>want</th>\n",
       "      <th>many</th>\n",
       "      <th>ask</th>\n",
       "      <th>good</th>\n",
       "      <th>find</th>\n",
       "      <th>work</th>\n",
       "      <th>go</th>\n",
       "      <th>include</th>\n",
       "      <th>important</th>\n",
       "      <th>know</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sell yourself first</td>\n",
       "      <td>yes</td>\n",
       "      <td>['Sell', 'yourself', 'first']</td>\n",
       "      <td>How to Sell Fine Art Online</td>\n",
       "      <td>3</td>\n",
       "      <td>1.725841</td>\n",
       "      <td>0.828910</td>\n",
       "      <td>Strong</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Before doing anything else, stop and sum up yo...</td>\n",
       "      <td>no</td>\n",
       "      <td>['Before', 'doing', 'anything', 'else,', 'stop...</td>\n",
       "      <td>How to Sell Fine Art Online</td>\n",
       "      <td>12</td>\n",
       "      <td>3.421057</td>\n",
       "      <td>0.806864</td>\n",
       "      <td>Strong</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Now, think about how to translate that to an o...</td>\n",
       "      <td>no</td>\n",
       "      <td>['Now,', 'think', 'about', 'how', 'to', 'trans...</td>\n",
       "      <td>How to Sell Fine Art Online</td>\n",
       "      <td>11</td>\n",
       "      <td>3.064425</td>\n",
       "      <td>0.857768</td>\n",
       "      <td>Strong</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                           sentence is_summary  \\\n",
       "0        0                                Sell yourself first        yes   \n",
       "1        0  Before doing anything else, stop and sum up yo...         no   \n",
       "2        0  Now, think about how to translate that to an o...         no   \n",
       "\n",
       "                                               words  \\\n",
       "0                      ['Sell', 'yourself', 'first']   \n",
       "1  ['Before', 'doing', 'anything', 'else,', 'stop...   \n",
       "2  ['Now,', 'think', 'about', 'how', 'to', 'trans...   \n",
       "\n",
       "                         title  sentence_len  tfidf_score  title_similarity  \\\n",
       "0  How to Sell Fine Art Online             3     1.725841          0.828910   \n",
       "1  How to Sell Fine Art Online            12     3.421057          0.806864   \n",
       "2  How to Sell Fine Art Online            11     3.064425          0.857768   \n",
       "\n",
       "  title_sim_categories  help  ...  want  many  ask  good  find  work  go  \\\n",
       "0               Strong     0  ...     0     0    0     0     0     0   0   \n",
       "1               Strong     0  ...     0     0    0     0     0     0   0   \n",
       "2               Strong     0  ...     0     0    0     0     0     0   0   \n",
       "\n",
       "   include  important  know  \n",
       "0        0          0     0  \n",
       "1        0          0     0  \n",
       "2        0          0     0  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikihow_subset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize `is_summary`\n",
    "wikihow_subset['summary_id'] = wikihow_subset['is_summary'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to keep (for now)\n",
    "features = wikihow_subset[['text_id','sentence', 'summary_id','sentence_len','tfidf_score', 'title_similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    83438\n",
       "1    16562\n",
       "Name: summary_id, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts of the classes to get an idea of the null accuracy\n",
    "features.summary_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83438"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_accuracy = 83438/(16562+83438)\n",
    "null_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Sentences into DTMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = features['sentence']\n",
    "y = features['summary_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "Xtrain_dtm = vectorizer.fit_transform(X_train)\n",
    "Xtrain_dtm_dense = Xtrain_dtm.toarray() \n",
    "Xtrain_dtm_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x291782 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 709570 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word counts to find min_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mat = Xtrain_dtm_dense[:1000]\n",
    "for document in feature_mat:\n",
    "    for word_index in range(len(document)):\n",
    "        if document[word_index] > 1:\n",
    "            document[word_index] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = pd.DataFrame(data=feature_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurence_by_doc = word_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "x = np.sort(word_occurence_by_doc)\n",
    "y = np.arange(1, len(x) + 1)/len(x)\n",
    "_ = plt.plot(x, y, marker='.', linestyle='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like min_df here is around 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x291782 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 709570 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm_tune1 = vectorizer.transform(X_test)\n",
    "X_test_dtm_tune1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing MultinomialNB with default params\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm\n",
    "nb.fit(Xtrain_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8515333333333334"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model performs slightly better than the null accuracy, we will need to do some parameter tuning and add some features in order to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24909,    51],\n",
       "       [ 4403,   637]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149      Contemporary short story masters 1950 Present ...\n",
       "36838                             Consider his perspective\n",
       "87607                              Use fluoride toothpaste\n",
       "26438                       Avoid foods with lots of carbs\n",
       "11468                              Recap your hCG solution\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# false positive summaries (non summaries incorrectly classified as summaries)\n",
    "X_test[y_pred_class > y_test].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19864    Cough or laugh during your ultrasound appointment\n",
       "63120                  Monitor newborns with extra caution\n",
       "35795                                 Drink more green tea\n",
       "575      Choose the same last name as your parent s sta...\n",
       "99089                                       Cut up carrots\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# false negative summaries (summaries incorrectly classified as non summaries)\n",
    "X_test[y_pred_class < y_test].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 2.96214600e-15],\n",
       "       [1.00000000e+00, 2.56879762e-35],\n",
       "       [7.53824578e-01, 2.46175422e-01],\n",
       "       ...,\n",
       "       [9.99993124e-01, 6.87550158e-06],\n",
       "       [1.00000000e+00, 1.60942824e-15],\n",
       "       [5.91404477e-01, 4.08595523e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Left: proba the class is 0, Right: proba the class is 1 \n",
    "nb.predict_proba(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940995354471917"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which words are more prone to be in a summary ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291782"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vocabulary of X_train\n",
    "X_train_tokens = vectorizer.get_feature_names()\n",
    "len(X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 1., 1., ..., 1., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of times each token appears in each class\n",
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 291782)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows represent classes, columns represent tokens\n",
    "nb.feature_count_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 1., 1., ..., 1., 1., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each token appears across all summary sentences\n",
    "non_summary_token_count = nb.feature_count_[0, :]\n",
    "non_summary_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each token appears across all non-summary sentences\n",
    "summary_token_count = nb.feature_count_[1, :]\n",
    "summary_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>non_summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00 and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00 it</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00 year</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         summary  non_summary\n",
       "token                        \n",
       "00           0.0          6.0\n",
       "00 and       0.0          1.0\n",
       "00 it        0.0          1.0\n",
       "00 year      0.0          2.0\n",
       "000          0.0         84.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'summary':summary_token_count, 'non_summary':non_summary_token_count}).set_index('token')\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>non_summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>do non</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>both habits</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of calm</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low diagnosis</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entire outer</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               summary  non_summary\n",
       "token                              \n",
       "do non             0.0          1.0\n",
       "both habits        0.0          1.0\n",
       "of calm            1.0          2.0\n",
       "low diagnosis      0.0          1.0\n",
       "entire outer       0.0          1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine 5 random DataFrame rows\n",
    "tokens.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58478., 11522.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of observations in each class\n",
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's tune the vectorizer and try Naive Bayes Again --> Parameter tuning w/ Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tune_1 = CountVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "Xtrain_dtm_tune1 = vectorizer_tune_1.fit_transform(X_train)\n",
    "Xtrain_dtm_dense_tune1 = Xtrain_dtm.toarray() \n",
    "Xtrain_dtm_dense_tune1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x20708 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 208442 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm_tune1 = vectorizer.transform(X_test)\n",
    "X_test_dtm_tune1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbtune1 = MultinomialNB()\n",
    "# train the model using X_train_dtm\n",
    "nbtune1.fit(Xtrain_dtm_tune1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class_tune1 = nbtune1.predict(X_test_dtm_tune1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8280666666666666"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class_tune1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this result we can see tha bigrams perform better than one word features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Naive Bayes with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarabouazzaoui/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "rf.fit(Xtrain_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class_rf = rf.predict(X_test_dtm)\n",
    "y_pred_class_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.832"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does slightly more poorly than the naive bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Representing text as tfidf score of different unigrams and bigrams \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True, norm='l2',sublinear_tf=True)\n",
    "tf_idf_vector = tfidf_transformer.fit_transform(Xtrain_dtm)\n",
    "\n",
    "#Converting the sparse matrix into a dense matrix\n",
    "tf_idf_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_transformer.transform(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MultinomialNB with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tfidf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_tfidf.fit(tf_idf_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tfidf_pred = nb_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8323666666666667"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, nb_tfidf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LDA to model topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "Xtrain_dtm = vectorizer.fit_transform(X_train)\n",
    "Xtrain_dtm_dense = Xtrain_dtm.toarray() \n",
    "Xtrain_dtm_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x299999 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 297005 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=30,random_state=42, max_iter=5)\n",
    "#fit model to the training data\n",
    "topic_results = lda.fit(Xtrain_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform both training and testing sets \n",
    "training_features = lda.transform(Xtrain_dtm)\n",
    "testing_features = lda.transform(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 299999)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 30)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate bow model with topic representation\n",
    "from scipy.sparse import hstack\n",
    "bow_topics = hstack((Xtrain_dtm, training_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 300029)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_topics.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feeding our newly concatenated bow with topic distributions to a multinomialNB classifier\n",
    "nb_bow_topics = MultinomialNB()\n",
    "nb_bow_topics.fit(training_features, y_train)\n",
    "bow_topics_pred = nb_bow_topics.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.832"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, bow_topics_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling + bow doesn't work as well as expected with default hyperparameters, In order to make it better we might need to do some hyperparameter tuning and/or add additional feature to help differetiate between a summary sentence and a non summary sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closer look at LDA results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = lda.fit_transform(Xtrain_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 299999)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df = pd.DataFrame(lda.components_, columns = vectorizer.get_feature_names())\n",
    "topics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 words for Topic #0\n",
      "['like', 'soap', 'blood', 'using', 'warm water', 'help', 'avoid', 'cup', 'mouth', 'use', 'warm', 'clean', 'rinse', 'skin', 'water']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #1\n",
      "['recommend', 'questions', 'exercise', 'treatment', 'family', 'pregnancy', 'time', 'apply', 'try', 'skin', 'use', 'ask doctor', 'help', 'ask', 'doctor']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #2\n",
      "['smoking', 'lemon juice', 'quit', 'medications', 'body', 'consult doctor', 'use', 'don', 'want', 'people', 'lemon', 'healthy', 'consult', 'cause', 'doctor']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #3\n",
      "['gum', 'treatment', 'prevent', 'doctor', 'home', 'area', 'brush teeth', 'avoid', 'body', 'need', 'try', 'help', 'brush', 'teeth', 'use']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #4\n",
      "['infection', 'test', 'healthy diet', 'especially', 'stress', 'try', 'doctor', 'diet', 'pregnancy', 'baby', 'healthy', 'body', 'like', 'use', 'help']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #5\n",
      "['stay', 'floss', 'support', 'water', 'baby', 'test', 'cause', 'body', 'days', 'health', 'foods', 'pregnancy', 'teeth', 'try', 'doctor']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #6\n",
      "['medical', 'braces', 'feel', 'teeth', 'drink plenty', 'baby', 'drink', 'help', 'like', 'pressure', 'pain', 'use', 'plenty', 'body', 'water']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #7\n",
      "['eat', 'brush', 'teeth', 'feel', 'sure', 'mouth', 'try', 'don', 'treatment', 'eating', 'just', 'make', 'want', 'floss', 'help']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #8\n",
      "['time', 'symptoms', 'try', 'blood vessels', 'vessels', 'teeth', 'avoid', 'temperature', 'reduce', 'skin', 'pressure', 'blood pressure', 'body', 'help', 'blood']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #9\n",
      "['symptoms', 'feel', 'wax', 'regular', 'teeth', 'exercise', 'drink', 'glasses', 'like', 'wash hands', 'water', 'hands', 'help', 'wash', 'day']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #10\n",
      "['hand', 'finger', 'area', 'apple cider', 'cider vinegar', 'like', 'cider', 'good', 'using', 'vinegar', 'apple', 'skin', 'help', 'use', 'don']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #11\n",
      "['food', 'drops', 'poison', 'coconut oil', 'cause', 'people', 'use', 'essential', 'coconut', 'tree oil', 'tea', 'tea tree', 'tree', 'skin', 'oil']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #12\n",
      "['make', 'need', 'baby', 'body', 'don', 'women', 'seconds', 'avoid', 'position', '30', 'day', 'help', 'pain', 'water', 'try']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #13\n",
      "['try', 'make sure', 'like', 'help', 'lip balm', 'away', 'sure', 'need', 'balm', 'lip', 'time', 'skin', 'make', 'use', 'water']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #14\n",
      "['try', 'appointment', 'don', 'teeth', 'brush', 'sleep', 'doctor', 'help', 'skin', 'blood', 'time', 'use', 'make sure', 'sure', 'make']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #15\n",
      "['health', 'time', 'body', 'birth', 'possible', 'help', 'water', 'pregnancy', 'doctor', 'make', 'skin', 'baking soda', 'baking', 'soda', 'use']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #16\n",
      "['make', 'stay hydrated', 'people', 'time', 'don', 'infection', 'medical', 'skin', 'muscles', 'use', 'hydrated', 'stay', 'help', 'symptoms', 'need']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #17\n",
      "['try', 'tests', 'healing', 'small', 'baby', 'exercises', 'blood', 'doctor', 'want', 'work', 'help', 'pay attention', 'day', 'pay', 'attention']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #18\n",
      "['sleep', 'pain', 'try', 'symptoms', 'like', 'good', 'body', 'day', 'teeth', 'time', 'foods', 'doctor', 'water', 'help', 'avoid']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #19\n",
      "['try', 'don', 'family', 'time', 'mouth', 'like', 'times', 'skin', 'use', 'repeat', 'make', 'teeth', 'day', 'drink', 'water']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #20\n",
      "['fatty', 'vegetables', 'fruits', 'products', 'vitamin', 'low', 'reduce', 'rich', 'dairy', 'like', 'fat', 'pain', 'help', 'eat', 'foods']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #21\n",
      "['pregnancy', 'want', 'foods', 'feel', 'vitamin', 'use', 'avoid', 'like', 'rest', 'area', 'mouth', 'cold', 'help', 'dry', 'skin']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #22\n",
      "['body', 'medication', 'people', 'health', 'use', 'help', 'good', 'try', 'water', 'like', 'baby', 'know', 'talk doctor', 'doctor', 'talk']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #23\n",
      "['make', 'avoid', 'time', 'try', 'cause', 'area', 'daily', 'compress', 'cold', 'exercise', 'water', 'hot', 'use', 'help', 'skin']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #24\n",
      "['blood', 'like', 'need', 'pregnancy', 'cause', 'place', 'baby', 'anti inflammatory', 'inflammatory', 'use', 'make', 'anti', 'pain', 'time', 'help']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #25\n",
      "['hydrogen peroxide', 'hydrogen', 'sore', 'ice', '20', 'peroxide', 'vera', 'aloe vera', 'gel', '10', 'aloe', '15', 'use', 'day', 'minutes']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #26\n",
      "['try', 'teeth', 'make', 'small', 'know', 'important', 'like', 'skin', 'mouth', 'pregnancy', 'help', 'work', 'symptoms', 'doctor', 'use']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #27\n",
      "['blood', 'genital warts', 'day', 'teeth', 'likely', 'genital', 'warts', 'time', 'baby', 'cause', 'need', 'bleeding', 'risk', 'help', 'doctor']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #28\n",
      "['fluoride', 'time', 'treatment', 'make', 'feel', 'need', 'don', 'skin', 'toothbrush', 'like', 'bad breath', 'teeth', 'bad', 'breath', 'use']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #29\n",
      "['birth', 'people', 'medical', 'week', 'treatment', 'repeat', 'try', 'skin', 'use', 'times day', 'exercise', 'need', 'day', 'help', 'times']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f'Top 15 words for Topic #{index}')\n",
    "    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = topic_results.argmax(axis = 1)\n",
    "topics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use word2vec to model Topics (Use those topics for prediction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(sent_collection):\n",
    "    tokenized_words = []\n",
    "    for sentence in sent_collection.values:\n",
    "        tokenized_words.append(sentence.split(' '))\n",
    "    return tokenized_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_train = word_tokenizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_words_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words_test = word_tokenizer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(tokenized_words_train, size=300, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4007246, 5167445)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reducing the epochs will decrease the computation time\n",
    "model.train(tokenized_words_train, total_examples=len(tokenized_words_train), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32833"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember: call to syn0 deprecated, new call: model.wv.vectors \n",
    "# The two datasets must be the same size\n",
    "max_dataset_size = len(model.wv.vectors)\n",
    "max_dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.6392549e-02,  3.4185508e-01, -7.0123136e-01, ...,\n",
       "        -7.0268047e-01,  6.9543946e-01, -3.7551209e-01],\n",
       "       [-1.2442722e+00, -7.5713813e-01,  1.0152851e+00, ...,\n",
       "         5.3939885e-01, -3.6989877e-01,  8.4144495e-02],\n",
       "       [-1.7692882e+00,  8.2116723e-01,  3.1036755e-01, ...,\n",
       "        -1.1512418e+00, -2.2056274e-01,  4.0052724e-01],\n",
       "       ...,\n",
       "       [-3.5188947e-02, -1.2058836e-03,  6.9164857e-03, ...,\n",
       "         6.4974194e-03, -1.0819169e-02,  6.2711886e-04],\n",
       "       [-1.2777128e-02, -3.5486629e-03,  1.0411546e-02, ...,\n",
       "        -2.4501206e-03,  3.3767486e-05,  5.7401322e-03],\n",
       "       [-1.4552454e-03, -7.1474947e-03, -9.6883103e-03, ...,\n",
       "        -1.4479854e-03,  9.3169054e-03,  4.9901307e-03]], dtype=float32)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardize vectors ?\n",
    "model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_train[:max_dataset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we have negative values, it is better to train a Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(random_state=42,max_iter=10000, solver='lbfgs', multi_class='multinomial').fit(model.wv.vectors, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.asarray(tokenized_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Talk', 'to', 'your', 'doctor', 'if', 'you', 'think', 'your', 'skin', 'condition', 'might', 'indicate', 'a', 'more', 'serious', 'health', 'concern', 'such', 'as', 'diabetes', 'or', 'Parkinson', 's', 'disease']),\n",
       "       list(['Most', 'of', 'the', 'time', 'you', 'can', 'administer', 'antibiotics', 'to', 'yourself', 'in', 'the', 'comfort', 'of', 'your', 'own', 'home,', 'but', 'if', 'your', 'infection', 'is', 'severe', 'enough', 'to', 'require', 'intravenous', 'antibiotics', 'in', 'order', 'to', 'deeply', 'penetrate', 'the', 'infected', 'area,', 'administration', 'must', 'be', 'done', 'in', 'a', 'hospital']),\n",
       "       list(['Cough', 'or', 'laugh', 'during', 'your', 'ultrasound', 'appointment']),\n",
       "       ...,\n",
       "       list(['Plan', 'to', 'replace', 'lost', 'fluids', 'and', 'make', 'Rationale', 'notes', 'accordingly']),\n",
       "       list(['This', 'will', 'be', 'best', 'for', 'you', 'in', 'the', 'long', 'term']),\n",
       "       list(['Replace', 'your', 'mouse', 'with', 'a', 'trackball'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-7d0a4e33a02b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    263\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "predict = logit.predict(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the score of the predictions\n",
    "score = logit.score(y_test[max_dataset_size:], predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring model similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71406215"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cosine similarity between two words in the vocalubulary \n",
    "model.wv.similarity('sleep', 'bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foods', 0.7835334539413452),\n",
       " ('stuck', 0.7295717000961304),\n",
       " ('braces', 0.7240970730781555),\n",
       " ('binging', 0.6891987323760986),\n",
       " ('breath', 0.6885989904403687),\n",
       " ('germs', 0.67889803647995),\n",
       " ('teeth', 0.6744553446769714),\n",
       " ('items', 0.668265700340271),\n",
       " ('particles', 0.6665470004081726),\n",
       " ('snacks', 0.662417471408844)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Additional features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>summary_id</th>\n",
       "      <th>sentence_len</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>title_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sell yourself first</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.725841</td>\n",
       "      <td>0.828910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Before doing anything else, stop and sum up yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3.421057</td>\n",
       "      <td>0.806864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Now, think about how to translate that to an o...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3.064425</td>\n",
       "      <td>0.857768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Be it the few words, Twitter allows you or an ...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>4.393483</td>\n",
       "      <td>0.815038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Bring out the most salient features of your cr...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3.500784</td>\n",
       "      <td>0.789926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                           sentence  summary_id  \\\n",
       "0        0                                Sell yourself first           1   \n",
       "1        0  Before doing anything else, stop and sum up yo...           0   \n",
       "2        0  Now, think about how to translate that to an o...           0   \n",
       "3        0  Be it the few words, Twitter allows you or an ...           0   \n",
       "4        0  Bring out the most salient features of your cr...           0   \n",
       "\n",
       "   sentence_len  tfidf_score  title_similarity  \n",
       "0             3     1.725841          0.828910  \n",
       "1            12     3.421057          0.806864  \n",
       "2            11     3.064425          0.857768  \n",
       "3            21     4.393483          0.815038  \n",
       "4            18     3.500784          0.789926  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
