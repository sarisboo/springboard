{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets from `analysis_no_features` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sparse matrices\n",
    "tf_idf_vectors = scipy.sparse.load_npz('./datasets/train_sparse_matrix.npz')\n",
    "X_test_tfidf = scipy.sparse.load_npz('./datasets/test_sparse_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tfidf feature names list\n",
    "# if file exists we have already pickled a list\n",
    "if os.path.isfile(\"tfidf_features.txt\"):\n",
    "    with open(\"tfidf_features.txt\", 'rb') as f:\n",
    "        tfidf_feature_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load X_train and X_test\n",
    "X_train = pd.read_csv('./datasets/wikihow_X_train.csv') \n",
    "X_test = pd.read_csv('./datasets/wikihow_X_test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load y_train and y_test \n",
    "y_train = pd.read_csv('./datasets/wikihow_y_train.csv', header = None) \n",
    "y_test = pd.read_csv('./datasets/wikihow_y_test.csv', header = None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing Dataframes for analysis \n",
    "Let's add some features to see if our model performs better. We start by adding the sentences length feture to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ft = X_train[['sentence','sentence_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the sentence lengths array \n",
    "sent_lengths = np.array(X_train_ft['sentence_len'].values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14],\n",
       "       [37],\n",
       "       [11],\n",
       "       ...,\n",
       "       [20],\n",
       "       [16],\n",
       "       [20]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting sentence lengths array to sparse matrix\n",
    "sparse_sent_lengths = scipy.sparse.csr_matrix(sent_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<70000x1 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 70000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_sent_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the two sparse arrays  \n",
    "X_train_feats = scipy.sparse.hstack([tf_idf_vectors,sparse_sent_lengths ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 95346)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the shape of the arrays  \n",
    "X_train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same with test set \n",
    "X_test_ft = X_test[['sentence','sentence_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the sentence lengths array \n",
    "sent_lengths_test = np.array(X_test_ft['sentence_len'].values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting sentence lengths array to sparse matrix\n",
    "sparse_sent_lengths_test = scipy.sparse.csr_matrix(sent_lengths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the two sparse arrays  \n",
    "X_test_feats = scipy.sparse.hstack([X_test_tfidf,sparse_sent_lengths_test ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 95346)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Cross-Validation with a Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "param_grid = {'alpha': (0.1, 1, 5, 10, 50)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_CV = GridSearchCV(estimator=nb, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
       "                                     fit_prior=True),\n",
       "             iid='warn', n_jobs=None, param_grid={'alpha': (0.1, 1, 5, 10, 50)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_CV.fit(X_train_feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.1}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_CV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Naive Bayes with tf idf scores and sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_ft = MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_ft.fit(X_train_feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= nb_ft.predict(X_test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[24669   291]\n",
      " [ 3159  1881]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Confusion Matrix: \\n {metrics.confusion_matrix(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.93     24960\n",
      "           1       0.87      0.37      0.52      5040\n",
      "\n",
      "    accuracy                           0.89     30000\n",
      "   macro avg       0.88      0.68      0.73     30000\n",
      "weighted avg       0.88      0.89      0.87     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Classification Report: \\n\\n {metrics.classification_report(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Naive Bayes with BoW and sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performing Cross Validation for the best combination of parameters\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "X_train_dtm = vectorizer.fit_transform(X_train['sentence'])\n",
    "X_train_dtm_dense = X_train_dtm.toarray() \n",
    "len(Xtrain_dtm_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features to X_train_dtm and X_test_dtm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<70000x100223 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1719852 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow_feats = scipy.sparse.hstack([X_train_dtm ,sparse_sent_lengths ])\n",
    "X_train_bow_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x100222 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 676598 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vectorizer.transform(X_test['sentence'])\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x100223 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 706598 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_bow_feats =scipy.sparse.hstack([X_test_dtm ,sparse_sent_lengths_test ])\n",
    "X_test_bow_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform MultinomialNB Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_bow = MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_bow.fit(X_train_bow_feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bow= nb_bow.predict(X_test_bow_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[23426  1534]\n",
      " [ 2056  2984]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Confusion Matrix: \\n {metrics.confusion_matrix(y_test, preds_bow)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     24960\n",
      "           1       0.66      0.59      0.62      5040\n",
      "\n",
      "    accuracy                           0.88     30000\n",
      "   macro avg       0.79      0.77      0.78     30000\n",
      "weighted avg       0.88      0.88      0.88     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Classification Report: \\n\\n {metrics.classification_report(y_test, preds_bow)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** - For both classes we have pretty high precision although for class 1 it is lower than the precision for the same class with th multinomialNB classifier with no features (0.77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall (sensitivity)** - is the ratio of correctly predicted positive observations to the all observations in actual class, with adding the sentence length feature, we get a higher recall for class 1 (summary) as opposed to the same classifier without this feature (0.41)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 score** - F1 is usually more useful than accuracy, especially since we have an uneven class distribution. In our case, F1 score is 0.88, but we notice the the F1 score for the summary class has improved (0.62) compared to the model without the `sentence_len` feature (0.53)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the recall for the summary class here is significantly better probably because the MultinomialNB classifier works best with integers (suitable for classification with discrete features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the `tfidf_score` to the MultinomialNB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ft = X_train[['sentence_len', 'tfidf_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tfidf = np.array(X_train_ft['tfidf_score'].values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting sentence tfidf array to sparse matrix\n",
    "sparse_sent_tfidf = scipy.sparse.csr_matrix(sent_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<70000x100224 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1789831 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow_feats_1 = scipy.sparse.hstack([X_train_bow_feats ,sparse_sent_tfidf])\n",
    "X_train_bow_feats_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for test set\n",
    "X_test_ft = X_test[['sentence','tfidf_score']]\n",
    "sent_tfidf_test = np.array(X_test_ft['tfidf_score'].values).reshape(-1, 1)\n",
    "#Converting sentence tfidf array to sparse matrix\n",
    "sparse_sent_tfidf_test = scipy.sparse.csr_matrix(sent_tfidf_test)\n",
    "#Concatenating the two sparse arrays  \n",
    "X_test_bow_feats_1 = scipy.sparse.hstack([X_test_bow_feats,sparse_sent_tfidf_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x100224 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 736591 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_bow_feats_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_bow_1 = MultinomialNB(alpha=0.1)\n",
    "nb_bow_1.fit(X_train_bow_feats_1, y_train)\n",
    "preds_bow_1= nb_bow_1.predict(X_test_bow_feats_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[23434  1526]\n",
      " [ 1903  3137]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Confusion Matrix: \\n {metrics.confusion_matrix(y_test, preds_bow_1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     24960\n",
      "           1       0.67      0.62      0.65      5040\n",
      "\n",
      "    accuracy                           0.89     30000\n",
      "   macro avg       0.80      0.78      0.79     30000\n",
      "weighted avg       0.88      0.89      0.88     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Classification Report: \\n\\n {metrics.classification_report(y_test, preds_bow_1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs even better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the `title_similarity` feature to the MultinomialNB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ft = X_train[['sentence_len', 'title_similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_len        3\n",
       "title_similarity    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ft[X_train_ft['title_similarity'] < 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only have 3 negative values, let's try making them 0 and see if the model is better this way\n",
    "X_train_ft[X_train_ft['title_similarity'] < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_len        0\n",
       "title_similarity    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ft[X_train_ft['title_similarity'] < 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_title_similarity = np.array(X_train_ft['title_similarity'].values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting sentence tfidf array to sparse matrix\n",
    "sparse_sent_title_similarity = scipy.sparse.csr_matrix(sent_title_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<70000x100225 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1859827 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow_feats_2 = scipy.sparse.hstack([X_train_bow_feats_1 ,sparse_sent_title_similarity])\n",
    "X_train_bow_feats_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for test set\n",
    "X_test_ft = X_test[['sentence','title_similarity']]\n",
    "sent_title_similarity_test = np.array(X_test_ft['title_similarity'].values).reshape(-1, 1)\n",
    "#Converting sentence tfidf array to sparse matrix\n",
    "sparse_sent_title_similarity_test = scipy.sparse.csr_matrix(sent_title_similarity_test)\n",
    "#Concatenating the two sparse arrays  \n",
    "X_test_bow_feats_2 = scipy.sparse.hstack([X_test_bow_feats_1,sparse_sent_title_similarity_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x100225 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 766589 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_bow_feats_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_bow_2 = MultinomialNB(alpha=0.1)\n",
    "nb_bow_2.fit(X_train_bow_feats_2, y_train)\n",
    "preds_bow_2= nb_bow_2.predict(X_test_bow_feats_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[23450  1510]\n",
      " [ 1799  3241]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Confusion Matrix: \\n {metrics.confusion_matrix(y_test, preds_bow_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     24960\n",
      "           1       0.68      0.64      0.66      5040\n",
      "\n",
      "    accuracy                           0.89     30000\n",
      "   macro avg       0.81      0.79      0.80     30000\n",
      "weighted avg       0.89      0.89      0.89     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Classification Report: \\n\\n {metrics.classification_report(y_test, preds_bow_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model give slighlty better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
